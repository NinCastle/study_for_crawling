{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[파이썬으로 특정 키워드를 포함사는 신문기사 웹크롤링](http://yoonpunk.tistory.com/4)  \n",
    "[파이썬 웹크롤러를 만드는 기초](http://f4strada4.cafe24.com/?p=1121)  \n",
    "[파이썬으로 웹 크롤러 만들기](http://www.hanbit.co.kr/channel/category/category_view.html?cms_code=CMS6168044195)  \n",
    "아래 내용은 상기 링크를 정리한것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bs4모듈에서 BeautifulSoup 함수를 불러온다.\n",
    "from bs4 import BeautifulSoup \n",
    "## URL을 열고 HTML을 읽는 모듈, urllib를 불러온다.\n",
    "import urllib.request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output file name\n",
    "output_file_name = 'naver_crawling.txt'\n",
    "## URL\n",
    "URL = 'http://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=732&oid=015&aid=0003956688'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find()와 findAll()\n",
    "find()와 findAll()은 BeautifulSoup에서 가장 자주 쓰이는 함수이다.  \n",
    "이 함수를 쓰면 HTML 페이지에서 원하는 태그를 다양한 속성에 따라 쉽게 필터링이 가능하다.  \n",
    "두 함수는 거의 비슷하며 함수의 정의 또한 비슷하다\n",
    "```python\n",
    "findAll(tag, attributes, recursive, text, limit, keywords)\n",
    "find(tag, attributes, recursive, text, keywords)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 크롤링 함수\n",
    "def get_text(URL):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    '''\n",
    "    해당 웹주소를 열고 Beautifulsoup으로 변수화 시킨다.\n",
    "    BeautifulSoup객체 생성자의 2번재 인자로 'lxml'을 사용해 기존 'html'방식 대식 'lxml'방식으로 파싱하고,\n",
    "    한글이 포함된 기사이기 때문에 encoding을 utf-8 방식을 채용했다.\n",
    "    ''' \n",
    "    soup = BeautifulSoup(source_code_from_URL, 'html', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    ## find_all \n",
    "    for t in soup.find_all('div', id=\"articleBodyContents\"):\n",
    "        text = text + str(t.find_all(text=True))\n",
    "    return text\n",
    "\n",
    "## 메인 함수\n",
    "def main():\n",
    "    open_output_file = open(output_file_name, 'w')\n",
    "    result_text = get_text(URL)\n",
    "    open_output_file.write(result_text)\n",
    "    open_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\conda\\conda\\envs\\text3.5\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Andrew\\AppData\\Local\\conda\\conda\\envs\\text3.5\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "메인 함수를 별도로 만들고 __name__을 이용해 main함수를 실행시켰다.\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "텍스트 정제 모듈\n",
    "특수기호 모두 제거\n",
    "'''\n",
    "import re\n",
    "\n",
    "## 입,출력 파일명\n",
    "input_file_name = 'naver_crawling.txt'\n",
    "output_file_name = 'naver_crawling_filter.txt'\n",
    "\n",
    "## fillter 함수\n",
    "def filter_text(text):\n",
    "    filter_text = re.sub('[a-zA-Z]', '', text)\n",
    "    filter_text = re.sub('[\\{\\}\\]\\/?.,;:|)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]','',fillter_text)\n",
    "    return filter_text\n",
    "\n",
    "# 메인함수\n",
    "def main():\n",
    "    read_file = open(input_file_name, 'r')\n",
    "    \n",
    "    write_file = open(output_file_name, 'w')\n",
    "    text = read_file.read()\n",
    "    print(\"---------수정 전---------\")\n",
    "    print(text)\n",
    "    text = fillter_text(text)\n",
    "    print()\n",
    "    print(\"---------수정 후---------\")\n",
    "    print(text)\n",
    "    write_file.write(text)\n",
    "    read_file.close()\n",
    "    write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------수정 전---------\n",
      "['\\n', ' 본문 내용 ', '\\n', ' TV플레이어 ', '\\n', ' // TV플레이어 ', '\\n', '\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\\n', '\\n', \"'유니버시티 블록체인 리서치 이니셔티브' 프로젝트 수행\", '고려대 본관. / 사진=한경 DB', '고려대가 글로벌 블록체인 기업 리플(Ripple)과 파트너십을 체결해 관련 전문인력 양성과 연구 프로젝트 수행에 나선다.', '리플은 블록체인을 이용해 저렴하면서 빠른 송금 서비스를 제공하는 회사로 자체 암호토큰 XRP를 이용한 서비스도 제공한다.', '7일 고려대에 따르면 이 대학 정보보호대학원 블록체인보안연구센터는 지난 4일 리플과 장기 연구 프로젝트 ‘유니버시티 블록체인 리서치 이니셔티브(University Blockchain Research Initiative)’ 수행을 위한 파트너십을 맺었다.', '세계 유수의 대학들과 함께 블록체인 및 암호화폐 관련 학문적·기술적 연구를 수행하도록 지원하는 연구 프로젝트다. 이 프로젝트 수행을 위해 리플과 파트너십을 맺은 대학은 MIT(매사추세츠공대) 프린스턴대 스탠퍼드대 등 전세계 17곳으로 국내에서는 고려대가 유일하다.', '파트너십 체결에 따라 고려대 블록체인보안연구센터는 블록체인 합의 알고리즘의 안전성, 스마트 콘트랙트(smart contract)의 안전성 등 블록체인 및 암호화폐에 필요한 보안 기술을 중점적으로 연구하며 리플은 연구 수행에 필요한 재정적·기술적 지원을 제공한다.', '아울러 사회적으로 부족한 블록체인 전문가를 양성하는 효과도 있을 것으로 예상된다.', '에릭 반 밀텐버그 리플 비즈니스 운영 수석부사장은 “‘유니버시티 블록체인 리서치 이니셔티브’는 대학이 블록체인 기술의 혁신에 있어 중요 역할을 할 수 있게 하는 디딤돌이 될 것”이라며 “블록체인 관련 산업체에서 일할 수 있는 인력을 길러내는 효과도 있을 것”이라고 말했다.', '이상진 고려대 정보보호대학원장도 “이번 파트너십 체결과 프로젝트 수행으로 리플의 실제 데이터에 접근해 분석해보는 기회를 갖게 됐다. 실용적인 블록체인 보안기술 개발에 중요한 역할을 할 것으로 기대된다”고 부연했다.', '김봉구 한경닷컴 기자 ', 'kbk9@hankyung.com', '기사제보 및 보도자료 ', 'open@hankyung.com', '[', '한경닷컴 바로가기', '] [', '글방', '] [', '모바일한경 구독신청', '] ', 'ⓒ 한국경제 & ', 'hankyung.com', ', 무단전재 및 재배포 금지\\n\\t', ' // 본문 내용 ', '\\n']\n",
      "\n",
      "---------수정 후---------\n",
      "[  본문 내용    플레이어     플레이어     오류를 우회하기 위한 함수 추가    유니버시티 블록체인 리서치 이니셔티브 프로젝트 수행 고려대 본관  사진한경  고려대가 글로벌 블록체인 기업 리플과 파트너십을 체결해 관련 전문인력 양성과 연구 프로젝트 수행에 나선다 리플은 블록체인을 이용해 저렴하면서 빠른 송금 서비스를 제공하는 회사로 자체 암호토큰 를 이용한 서비스도 제공한다 7일 고려대에 따르면 이 대학 정보보호대학원 블록체인보안연구센터는 지난 4일 리플과 장기 연구 프로젝트 ‘유니버시티 블록체인 리서치 이니셔티브   ’ 수행을 위한 파트너십을 맺었다 세계 유수의 대학들과 함께 블록체인 및 암호화폐 관련 학문적·기술적 연구를 수행하도록 지원하는 연구 프로젝트다 이 프로젝트 수행을 위해 리플과 파트너십을 맺은 대학은 매사추세츠공대 프린스턴대 스탠퍼드대 등 전세계 17곳으로 국내에서는 고려대가 유일하다 파트너십 체결에 따라 고려대 블록체인보안연구센터는 블록체인 합의 알고리즘의 안전성 스마트 콘트랙트 의 안전성 등 블록체인 및 암호화폐에 필요한 보안 기술을 중점적으로 연구하며 리플은 연구 수행에 필요한 재정적·기술적 지원을 제공한다 아울러 사회적으로 부족한 블록체인 전문가를 양성하는 효과도 있을 것으로 예상된다 에릭 반 밀텐버그 리플 비즈니스 운영 수석부사장은 “‘유니버시티 블록체인 리서치 이니셔티브’는 대학이 블록체인 기술의 혁신에 있어 중요 역할을 할 수 있게 하는 디딤돌이 될 것”이라며 “블록체인 관련 산업체에서 일할 수 있는 인력을 길러내는 효과도 있을 것”이라고 말했다 이상진 고려대 정보보호대학원장도 “이번 파트너십 체결과 프로젝트 수행으로 리플의 실제 데이터에 접근해 분석해보는 기회를 갖게 됐다 실용적인 블록체인 보안기술 개발에 중요한 역할을 할 것으로 기대된다”고 부연했다 김봉구 한경닷컴 기자  9 기사제보 및 보도자료   [ 한경닷컴 바로가기  [ 글방  [ 모바일한경 구독신청   ⓒ 한국경제     무단전재 및 재배포 금지   본문 내용  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text3.5",
   "language": "python",
   "name": "text3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
