{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습목표\n",
    "1. HTTP(HyperText Transport Protocol) 개념 이해\n",
    "2. Crawling 개념 이해\n",
    "3. BeautifulSoup module을 이용한 Crawling 이해 및 실제 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HTTP(HyperText Tranport Protocol)\n",
    "### 1.1 프로토콜 : 네트워크 통신 규약\n",
    "\n",
    "* 인터넷 프로토콜 : TCP 및 IP 프로토콜이 핵심, TCP/IP 프로토콜\n",
    "* 이더넷: 네트워크 모듈\n",
    "* IP 프로토콜 : 컴츄터 주소를 찾는 프로토콜\n",
    "* TCP 프로토콜 : 컴퓨터간 신뢰성이 있는 데이터 전송을 지원하는 프로토콜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- form tag의 이해\n",
    "    * form tag는 클라이언트에서 서버로 데이터 전송을 위해 사용.\n",
    "    * form tag의 대표적인 사용 예로 로그인을 들수 있다.\n",
    "        * Id와 password를 서버로 전송해야 하는데 이때 form data를 이용\n",
    "    * form tag는 여러 속성을 가지고 있으나 제일 중요한 것은 아래 두 속성\n",
    "        * action: 수신 대상\n",
    "        * method: 전송 방식\n",
    "            ```            \n",
    "           1. <form action =\"result.jsp\" method=\"post\">\n",
    "           2. ....\n",
    "           3. </form>\n",
    "            \n",
    "            ```\n",
    "        * 위 코드를 예로 들면 form의 수신대상은 result.jsp이고, 전송방식은 post방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- HTTP 프로토콜 : WWW(웹)상에서 문서 전송을 위한 프로토콜**\n",
    "   - request(요청)/ response(응답)으로 구성\n",
    "    \n",
    "        * browser(클라이언트)가 요청하면 web server(서버)가 HTML 파일이나 다른 자원(이미지, 텍스트 등을)을 응답으로 전송\n",
    "        * request의 형태에는 대표적으로 GET/ POST가 있음\n",
    "            * <font color=\"Green\">GET 방식</font> : 데이터 전달을 URL내에서 한다\n",
    "                * ex:\n",
    "                ```\n",
    "                https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0\n",
    "                \n",
    "                ```\n",
    "            * <font color=\"Green\">POST 방식</font> : 데이터 전송을 form 태그를 통해서 사용(클라이언트에 직접적으로 노출되지 않는다.)\n",
    "                * ex: \n",
    "                ```\n",
    "                ID, 비밀번호 전달의 경우\n",
    "                \n",
    "                ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다시말해서 \n",
    "    * GET은 쪽지에 적어서 보내기\n",
    "    * POST는 보안가방안에 적어보내기 라고 생각하면된다.\n",
    "        * 간단히 비교하면 아래와 같다\n",
    "\n",
    "| **Method** | **속도** | **보안** | **전송량** |\n",
    "|:-----------:|:---------:|:----------:|:---------:|\n",
    "| GET  | 빠름 | 없음 | 제한적(255 Byts) |\n",
    "| POST | 느림 | 있음 | 제한 없음 |       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Crawling 이란?\n",
    "* Web상에 존재하는 Contents를 수집하는 작업 (프로그래밍으로 자동화 가능)\n",
    "    1. HTML 페이지를 **가져와서**, HTML/CSS등을 **파싱**하고, 필요한 데이터만 추출하는 기법\n",
    "    2. Open API(Rest API)를 제공하는 서비스에 Open API를 호출해서, 받은 데이터중 필요하만 데이터를 추출하는 기법\n",
    "    3. **Selenium**등 브라우저를 프로그래밍으로 조작해서, 필요한 데이터만 추출하는 기법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Crawling 예제\n",
    "**3.1. BeautifulSoup 라이브러리를 활용한 초간단 예제**\n",
    "   - HTML의 태그를 파싱해서 필요한 데이터만 추출하는 함수를 제공하는 라이브러리\n",
    "   - [BeautifulSoup 라이브러리 페이지](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "   - 설치방법(linux, Anaconda가 설치된 windows)\n",
    "       - pip install bs4\n",
    "   - [참고: BeautifulSoup 4 API Guide](http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토 | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) requests 라이브러리를 활용한 HTML 페이지 요청\n",
    "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출 할 수 있음\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "# print(res.content)\n",
    "\n",
    "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)\n",
    "# 2-1) BeautifulSoup 파싱방법\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "# 3) 필요한 데이터 검색\n",
    "title = soup.find('title')\n",
    "# print(title)\n",
    "\n",
    "# 4) 필요한 데이터 추출\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. BeaufifulSoup 라이브러리 활용 주요 예제**\n",
    "   - find()와 find_all() 메서드 사용법 이해하기\n",
    "   - find() : 가장먼저 검색되는 태그 반환\n",
    "   - find_all() : 전체 태그 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]크롤링 이란?</h1>\n",
      "[1]크롤링 이란?\n",
      "[1]크롤링 이란?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html> \\\n",
    "    <body> \\\n",
    "        <h1 id='title'>[1]크롤링 이란?</h1> \\\n",
    "        <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
    "        <p id='body' align='enter'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
    "    </body> \\\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#태그로 검색 방법\n",
    "title_data = soup.find('h1')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 가장먼저 검색되는 태그를 반환\n",
    "paragraph_data = soup.find('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
